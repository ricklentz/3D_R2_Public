{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Public 1.5B OpenGPT-2 GPU Inference ",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricklentz/3D_R2_Public/blob/master/1_5B_OpenGPT_2_GPU_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKbiVNRJSWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cd47f0d0-cc80-4b76-a9f9-6ec2f96e36ef"
      },
      "source": [
        "!git clone https://github.com/rowanz/grover.git\n",
        "%cd /content/grover"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'grover'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 76 (delta 25), reused 66 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n",
            "/content/grover\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0H-Gkg0XV-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "cc610887-9e5c-46da-e9b1-d0bc52c2b1d5"
      },
      "source": [
        "!python3 -m pip install regex jsonlines \n",
        "!python3 -m pip install -U tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 1.9MB/s \n",
            "\u001b[?25hCollecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609236 sha256=d85aba2bbc9341be7ccd3f929660b892e2378d4e932852d23a3fe589052f5c7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, jsonlines\n",
            "Successfully installed jsonlines-1.2.0 regex-2019.8.19\n",
            "Collecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/83/06029af22fe06b8a7be013aeae5e104b3ed26867e5d4ca91408b30aa602e/tqdm-4.34.0-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.4MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.34.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip52YU9X5BwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b9a7c79b-9913-4ee1-b479-35dc72ba0844"
      },
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from apiclient.http import MediaIoBaseDownload\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "import os  \n",
        "import io\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "model_type = 'mega'\n",
        "model_path = '/content/grover/models'\n",
        "model_dir = os.path.join(model_path, model_type)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "NUMBER = 800000\n",
        "MODEL = \"grover_mega_owt_fix\"\n",
        "\n",
        "local_file_ids = ['1t1B5JfjolytwSSUAGOZCnstVlaL2Bg25', '1kojGap2kXzkJBWtwIZtOXgeV3IWnNMtO', '1FITtxBwJsagKfaLz9tgsguHbbdMCyeQw']\n",
        "local_filenames = ['.data-00000-of-00001', '.index', '.meta']\n",
        "for ext, id_ in zip(local_filenames, local_file_ids):\n",
        "    ext = str(NUMBER) + ext\n",
        "    filename = '%s/%s/model.ckpt.%s' % (model_path, model_type, ext) \n",
        "    \n",
        "    request = drive_service.files().get_media(fileId=id_)\n",
        "    with open(filename, 'wb') as f:\n",
        "      downloader = MediaIoBaseDownload(f, request, chunksize=100*1024*1024)\n",
        "      done = False\n",
        "      pbar = tqdm(total=100, desc='%s' % ext)\n",
        "      progress = 0\n",
        "      while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "        new_progress = int(status.progress() * 100)\n",
        "        pbar.update(new_progress - progress)\n",
        "        progress = new_progress\n",
        "\n",
        "      pbar.close()\n",
        "      print('Downloaded %s' % filename)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eb5eb09e4414cc594b83b2f5ca52541",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='800000.data-00000-of-00001', style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloaded /content/grover/models/mega/model.ckpt.800000.data-00000-of-00001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f2e8784ad1c477492a2e4bcfbd49444",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='800000.index', style=ProgressStyle(description_width='initial…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloaded /content/grover/models/mega/model.ckpt.800000.index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "797388b64bbb492da5676aa9eecee5f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='800000.meta', style=ProgressStyle(description_width='initial'…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloaded /content/grover/models/mega/model.ckpt.800000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmQXE6pUhabg",
        "colab_type": "code",
        "outputId": "469ca4f7-312b-46f9-8e3b-358ecce32d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/grover"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/grover\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGQI4BECpHiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sample/encoder.py', 'w') as f:\n",
        "  f.write(\"\"\"\\n\\\"\\\"\\\"Byte pair encoding utilities\\n\\nSome functions are adapted from OpenAI but with modifications\\n\\nhttps://github.com/openai/gpt-2\\n\\\"\\\"\\\"\\n\\nimport os\\nimport json\\nimport regex as re\\nfrom functools import lru_cache\\nimport tensorflow as tf\\nimport random\\nimport numpy as np\\n\\n\\n@lru_cache()\\ndef bytes_to_unicode():\\n    \\\"\\\"\\\"\\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\\n    The reversible bpe codes work on unicode strings.\\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\\n    \\\"\\\"\\\"\\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\\n    cs = bs[:]\\n    n = 0\\n    for b in range(2 ** 8):\\n        if b not in bs:\\n            bs.append(b)\\n            cs.append(2 ** 8 + n)\\n            n += 1\\n    cs = [chr(n) for n in cs]\\n    return dict(zip(bs, cs))\\n\\n\\ndef get_pairs(word):\\n    \\\"\\\"\\\"Return set of symbol pairs in a word.\\n\\n    Word is represented as tuple of symbols (symbols being variable-length strings).\\n    \\\"\\\"\\\"\\n    pairs = set()\\n    prev_char = word[0]\\n    for char in word[1:]:\\n        pairs.add((prev_char, char))\\n        prev_char = char\\n    return pairs\\n\\n\\nclass Encoder:\\n    def __init__(self, encoder, bpe_merges, errors='replace'):\\n        # self.encoder = {k: v + 1 for k, v in encoder.items()}\\n        # self.encoder['<|padding|>'] = 0\\n        # self.padding = 0\\n        #\\n        # del self.encoder['<|endoftext|>']\\n        #\\n        # for special_token_type in ['domain', 'date', 'authors', 'title', 'article', 'summary']:\\n        #     setattr(self, f'begin_{special_token_type}', len(self.encoder))\\n        #     self.encoder[f'<|begin{special_token_type}|>'] = len(self.encoder)\\n        #\\n        #     setattr(self, f'end_{special_token_type}', len(self.encoder))\\n        #     self.encoder[f'<|endof{special_token_type}|>'] = len(self.encoder)\\n        #\\n        # # This will be used if we want to combine short articles.\\n        # self.reset_context = len(self.encoder)\\n        # self.encoder['<|resetcontext|>'] = len(self.encoder)\\n\\n        self.encoder = encoder\\n        self.endoftext = self.encoder['<|endoftext|>']\\n\\n        ################################## END OF SPECIAL TOKENS TO ADD\\n\\n        self.decoder = {v: k for k, v in self.encoder.items()}\\n        self.errors = errors  # how to handle errors in decoding\\n        self.byte_encoder = bytes_to_unicode()\\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\\n        self.cache = {}\\n\\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\\n        self.pat = re.compile(r\\\"\\\"\\\"'s|'t|'re|'ve|'m|'ll|'d| ?\\\\p{L}+| ?\\\\p{N}+| ?[^\\\\s\\\\p{L}\\\\p{N}]+|\\\\s+(?!\\\\S)|\\\\s+\\\"\\\"\\\")\\n\\n    def bpe(self, token):\\n        if token in self.cache:\\n            return self.cache[token]\\n        word = tuple(token)\\n        pairs = get_pairs(word)\\n\\n        if not pairs:\\n            return token\\n\\n        while True:\\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\\n            if bigram not in self.bpe_ranks:\\n                break\\n            first, second = bigram\\n            new_word = []\\n            i = 0\\n            while i < len(word):\\n                try:\\n                    j = word.index(first, i)\\n                    new_word.extend(word[i:j])\\n                    i = j\\n                except:\\n                    new_word.extend(word[i:])\\n                    break\\n\\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\\n                    new_word.append(first + second)\\n                    i += 2\\n                else:\\n                    new_word.append(word[i])\\n                    i += 1\\n            new_word = tuple(new_word)\\n            word = new_word\\n            if len(word) == 1:\\n                break\\n            else:\\n                pairs = get_pairs(word)\\n        word = ' '.join(word)\\n        self.cache[token] = word\\n        return word\\n\\n    def encode(self, text):\\n        bpe_tokens = []\\n        for token in re.findall(self.pat, text):\\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\\n        return bpe_tokens\\n\\n    def decode(self, tokens):\\n        text = ''.join([self.decoder[token] for token in tokens])\\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\\n        return text\\n\\n    def __len__(self):\\n        return len(self.encoder)\\n\\n    @property\\n    def special_tokens_onehot(self):\\n        \\\"\\\"\\\" Return the IDs of all special tokens\\\"\\\"\\\"\\n        return [(self.decoder[i].startswith('<|') and self.decoder[i].endswith('|>')) for i in range(len(self))]\\n\\n\\ndef get_encoder():\\n    directory_name = os.path.dirname(__file__)\\n    with open(os.path.join(directory_name, 'encoder.json'), 'r') as f:\\n        encoder = json.load(f)\\n    with open(os.path.join(directory_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\\n        bpe_data = f.read()\\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\\\n')[1:-1]]\\n    return Encoder(\\n        encoder=encoder,\\n        bpe_merges=bpe_merges,\\n    )\\n\\n\\n##############################################################\\n# TURN SOMETHING INTO THE RIGHT FORMAT FOR AN EXAMPLE\\n##############################################################\\ndef _tokenize_article_pieces(encoder, item):\\n    \\\"\\\"\\\"\\n    Turn the article into tokens\\n    NOTE: in hindsight I kinda messed up here because the first token is always represented as a BPE continuation\\n    rather than an initial token in its own right. whoops....\\n\\n    :param item: Contains things that need to be tokenized\\n\\n\\n    fields are ['domain', 'date', 'authors', 'title', 'article', 'summary']\\n    :return: dict\\n    \\\"\\\"\\\"\\n    # article_pieces = {\\n    #     'article': [encoder.begin_article] + encoder.encode(item['text']) + [encoder.end_article],\\n    #     'domain': [encoder.begin_domain] + encoder.encode(item['domain']) + [encoder.end_domain],\\n    #     'title': [encoder.begin_title] + encoder.encode(item['title']) + [encoder.end_title],\\n    # }\\n    # # 4/6: Attach the summary too, why the hell not\\n    # if item['summary'] and len(item['summary']) > 50:\\n    #     article_pieces['summary'] = [encoder.begin_summary] + encoder.encode(item['summary']) + [encoder.end_summary]\\n    #\\n    # # 5/6: date\\n    # date_split = item['publish_date'].split('-')\\n    # assert len(date_split) == 3\\n    # assert date_split[0].isdigit()\\n    #\\n    # date_txt = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\\n    #             'August', 'September', 'October', 'November', 'December'][int(date_split[0]) - 1] + ' {}, {}'.format(\\n    #     date_split[1], date_split[2])\\n    # article_pieces['date'] = [encoder.begin_date] + encoder.encode(date_txt) + [encoder.end_date]\\n    #\\n    # # 6/6: authors\\n    # authors = ', '.join(item['authors'])\\n    # if len(authors) > 5:\\n    #     article_pieces['authors'] = [encoder.begin_authors] + encoder.encode(authors) + [encoder.end_authors]\\n    return encoder.encode(item) + [encoder.endoftext]\\n\\n\\ndef _cut_tokens_to_add_stuff(tokens, stuff_to_add, desired_size, padding_token):\\n    \\\"\\\"\\\"\\n    The idea behind this function is to take away tokens from `tokens' such that tokens[:LENGTH] + stuff_to_add becomes\\n    exactly at the right size (desired_size).\\n\\n    :param tokens:\\n    :param stuff_to_add:\\n    :param desired_size:\\n    :return:\\n    \\\"\\\"\\\"\\n    if len(tokens) >= desired_size:\\n        return tokens\\n\\n    # no way we can add this stuff\\n    if len(stuff_to_add) >= desired_size:\\n        return tokens\\n\\n    if (len(tokens) + len(stuff_to_add)) <= desired_size:\\n        return tokens + stuff_to_add\\n\\n    # Otherwise we'll have to actually cut\\n    tokens = tokens[:(desired_size - len(stuff_to_add) - 1)]\\n    tokens.append(padding_token)\\n    tokens.extend(stuff_to_add)\\n    return tokens\\n\\n\\ndef tokenize_for_grover_training(encoder, item, desired_size=1024, unconditional_prob=0.35, metadata_dropout_prob=0.1,\\n                                 cut_prob=0.2):\\n    \\\"\\\"\\\"\\n    Not only will we tokenize an item with a BPE encoder, but we'll also put it in a nice format for language modeling.\\n    The goal is to MINIMIZE PADDING. If we don't fill up the desired size of 1024 tokens then we're wasting compute.\\n\\n    The canonical order is\\n\\n    DOMAIN DATE AUTHORS TITLE ARTICLE SUMMARY\\n\\n\\n    :param encoder:\\n    :param item: Contains things like\\n          {\"url\": \"https://www.advocate.com/node/1010911\",\\n          \"timestamp\": \"20180118211607\",\\n           \"url_used\": \"https://web.archive.org/web/20180118211607id_/https://www.advocate.com/node/1010911\",\\n           \"domain\": \"advocate.com\",\\n           \"title\": \"Report: One-Third of Trump's Judicial Picks Are Anti-LGBT\",\\n           \"text\": ....\\n           \"summary\": ....\\n           \"authors\": list\\n           \"publish_date\": ...\\n           }\\n    :param desired_size: the goal for how long the span will be\\n    :param unconditional_prob: The probability that we will generate JUST THE TEXT first.\\n    :param metadata_dropout_prob: The probability that we will drop out each item of metadata\\n    :param cut_prob: The probability that, if we're already over the desired size, we'll cut the article and start\\n                    predicting metadata before the desired_size window ends.\\n    :return:\\n    \\\"\\\"\\\"\\n    # Get all the bits and pieces\\n    tokens = _tokenize_article_pieces(encoder, item)\\n    # canonical_metadata_order = ['domain', 'date', 'authors', 'title']\\n    #\\n    # # unconditional_prob is probability we only generate the text first, without any metadata\\n    # switch = random.random()\\n    # if switch < unconditional_prob:\\n    #     assignments = {'article': 'a'}\\n    #     chunk_a = article_pieces.pop('article')\\n    #     chunk_b = []\\n    #     for x in canonical_metadata_order + ['summary']:\\n    #         if random.random() > metadata_dropout_prob:\\n    #             chunk_b.extend(article_pieces.pop(x, []))\\n    #             assignments[x] = 'b'\\n    # elif switch < 0.5:\\n    #     # Put everything in chunk_a, without dropout\\n    #     assignments = {}\\n    #     chunk_a = []\\n    #     chunk_b = []\\n    #     for x in canonical_metadata_order + ['article', 'summary']:\\n    #         chunk_a.extend(article_pieces.pop(x, []))\\n    #         assignments[x] = 'a'\\n    # else:\\n    #     assignments = {}\\n    #     chunk_a = []\\n    #     chunk_b = []\\n    #     for k in canonical_metadata_order + ['article', 'summary']:\\n    #         if random.random() < metadata_dropout_prob and k not in ('article', 'title'):\\n    #             pass\\n    #         elif random.random() < 0.5:\\n    #             if k != 'summary':\\n    #                 chunk_a.extend(article_pieces.pop(k, []))\\n    #                 assignments[k] = 'a'\\n    #         else:\\n    #             chunk_b.extend(article_pieces.pop(k, []))\\n    #             assignments[k] = 'b'\\n    #\\n    # if (len(chunk_a) + len(chunk_b)) <= desired_size:\\n    #     return chunk_a + chunk_b\\n    #\\n    # if (assignments.get('article', '') == 'a') and (len(chunk_b) > 0) and (random.random() < cut_prob):\\n    #     return _cut_tokens_to_add_stuff(chunk_a, chunk_b, desired_size, encoder.padding)\\n    #\\n    # tokens = chunk_a + chunk_b\\n\\n    return tokens\\n\\n\\ndef detokenize(encoder, tokens):\\n    return encoder.decode(tokens)\\n\\n\\n#######################################\\n\\ndef create_int_feature(values):\\n    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\\n    return feature\\n\\n\\ndef sliding_window(article, max_seq_length):\\n    \\\"\\\"\\\"\\n    Randomly sample some spans. It's a simple approximation of sliding window\\n    :param tokens:\\n    :param max_seq_length:\\n    :return:\\n    \\\"\\\"\\\"\\n    # if it's shorter, no need for this\\n    if len(article['input_ids']) <= max_seq_length:\\n        amount_to_pad = max_seq_length - len(article['input_ids'])\\n        yield article\\n        return\\n\\n    num_spans = len(article['input_ids']) - max_seq_length + 1\\n    weights = np.ones(num_spans, dtype=np.float32)\\n    # weights[0] = max_seq_length\\n    weights /= weights.sum()\\n\\n    num_to_yield = int(0.5 + len(article['input_ids']) / max_seq_length)\\n    starts = np.random.choice(num_spans, size=num_to_yield, replace=False, p=weights)\\n\\n    input_ids = article.pop('input_ids')\\n    for i in starts.tolist():\\n        article['input_ids'] = input_ids[i:(i + max_seq_length)]\\n        yield article\\n\\n# def sliding_window(article, max_seq_length, pad_token):\\n#     \\\"\\\"\\\"\\n#     Randomly sample some spans. It's a simple approximation of sliding window\\n#     :param tokens:\\n#     :param max_seq_length:\\n#     :return:\\n#     \\\"\\\"\\\"\\n#     # if it's shorter, no need for this\\n#     if len(article['input_ids']) <= max_seq_length:\\n#         amount_to_pad = max_seq_length - len(article['input_ids'])\\n#         article['input_ids'].extend([pad_token] * amount_to_pad)\\n#         yield article\\n#         return\\n#\\n#     num_spans = len(article['input_ids']) - max_seq_length + 1\\n#     weights = np.ones(num_spans, dtype=np.float32)\\n#     # weights[0] = max_seq_length\\n#     weights /= weights.sum()\\n#\\n#     num_to_yield = int(0.5 + len(article['input_ids']) / max_seq_length)\\n#     starts = np.random.choice(num_spans, size=num_to_yield, replace=False, p=weights)\\n#\\n#     input_ids = article.pop('input_ids')\\n#     for i in starts.tolist():\\n#         article['input_ids'] = input_ids[i:(i + max_seq_length)]\\n#         yield article\\n\\n\\ndef format_context(encoder, news_article, target):\\n    \\\"\\\"\\\"\\n    Generates a news article given some partial information\\n    :param news_article: Contains context\\n    :param target: What we want to get an answer for.\\n    :return:\\n    \\\"\\\"\\\"\\n    canonical_metadata_order = ['domain', 'date', 'authors', 'title', 'article']\\n    tokens = []\\n    for metadata_category in canonical_metadata_order:\\n        metadata = news_article.get(metadata_category, '').strip()\\n\\n        # This MIGHT BE needed because I think during training time we never saw empty articles\\n        # if metadata or ((metadata_category == 'article') and target != 'article'):\\n        if (metadata_category == 'article') and (target != 'article'):\\n            metadata = news_article.get('title', '')  # Just copy from the title maybe?\\n\\n        if metadata:\\n            tokens.append(encoder.__dict__[f'begin_{metadata_category}'])\\n            tokens.extend(encoder.encode(metadata))\\n            tokens.append(encoder.__dict__[f'end_{metadata_category}'])\\n\\n    assert target in (canonical_metadata_order + ['summary'])\\n    tokens.append(encoder.__dict__[f'begin_{target}'])\\n    return tokens\\n\\ndef extract_generated_target(output_tokens, encoder, target):\\n    \\\"\\\"\\\"\\n    Given some tokens that were generated, extract the target\\n    :param output_tokens: [num_tokens] thing that was generated\\n    :param encoder: how they were encoded\\n    :param target: the piece of metadata we wanted to generate!\\n    :return:\\n    \\\"\\\"\\\"\\n    # Filter out first instance of start token\\n    assert output_tokens.ndim == 1\\n\\n    start_ind = 0\\n    end_ind = output_tokens.shape[0]\\n\\n    return {\\n        'extraction': encoder.decode(output_tokens[start_ind:end_ind]),\\n        'start_ind': start_ind,\\n        'end_ind': end_ind,\\n    }\\n\\n# def extract_generated_target(output_tokens, encoder, target):\\n#     \\\"\\\"\\\"\\n#     Given some tokens that were generated, extract the target\\n#     :param output_tokens: [num_tokens] thing that was generated\\n#     :param encoder: how they were encoded\\n#     :param target: the piece of metadata we wanted to generate!\\n#     :return:\\n#     \\\"\\\"\\\"\\n#     # Filter out first instance of start token\\n#     assert output_tokens.ndim == 1\\n#\\n#     start_tokens = output_tokens == encoder.__dict__[f'begin_{target}']\\n#     if np.any(start_tokens):\\n#         start_ind = np.argmax(start_tokens) + 1\\n#     else:\\n#         start_ind = 0\\n#\\n#     end_tokens = output_tokens == encoder.__dict__[f'end_{target}']\\n#     if np.any(end_tokens):\\n#         end_ind = np.argmax(end_tokens)\\n#     else:\\n#         end_ind = output_tokens.shape[0]\\n#\\n#     return {\\n#         'extraction': encoder.decode(output_tokens[start_ind:end_ind]),\\n#         'start_ind': start_ind,\\n#         'end_ind': end_ind,\\n#     }\\n\\n\\nif __name__ == '__main__':\\n    encoder = get_encoder()\\n    print(\"VOCAB SIZE IS {}\".format(len(encoder.encoder)))\\n\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZZlKUxYnFFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Supports repetitions with -samples <number>\n",
        "with open('sample/contextual_generate_cli.py', 'w') as f:\n",
        "  f.write(\"\"\"\\nimport tensorflow as tf\\nimport numpy as np\\nimport sys\\nimport json\\nimport sys\\n\\nsys.path.append('../')\\nfrom lm.modeling import GroverModel, GroverConfig, _top_p_sample, sample\\nfrom sample.encoder import get_encoder, format_context, _tokenize_article_pieces, extract_generated_target\\nfrom tqdm import tqdm\\n\\nimport argparse\\n\\nparser = argparse.ArgumentParser(description='Contextual generation (aka given some metadata we will generate articles')\\nparser.add_argument(\\n    '-metadata_fn',\\n    dest='metadata_fn',\\n    type=str,\\n    help='Path to a JSONL containing metadata',\\n)\\nparser.add_argument(\\n    '-out_fn',\\n    dest='out_fn',\\n    type=str,\\n    help='Out jsonl, which will contain the completed jsons',\\n)\\nparser.add_argument(\\n    '-input',\\n    dest='input',\\n    type=str,\\n    help='Text to complete',\\n)\\nparser.add_argument(\\n    '-model_config_fn',\\n    dest='model_config_fn',\\n    default='../lm/configs/base.json',\\n    type=str,\\n    help='Configuration JSON for the model',\\n)\\nparser.add_argument(\\n    '-model_ckpt',\\n    dest='model_ckpt',\\n    default='../models/base/model.ckpt',\\n    type=str,\\n    help='checkpoint file for the model',\\n)\\nparser.add_argument(\\n    '-target',\\n    dest='target',\\n    default='article',\\n    type=str,\\n    help='What to generate for each item in metadata_fn. can be article (body), title, etc.',\\n)\\nparser.add_argument(\\n    '-batch_size',\\n    dest='batch_size',\\n    default=1,\\n    type=int,\\n    help='How many things to generate per context. will split into chunks if need be',\\n)\\nparser.add_argument(\\n    '-num_folds',\\n    dest='num_folds',\\n    default=1,\\n    type=int,\\n    help='Number of folds. useful if we want to split up a big file into multiple jobs.',\\n)\\nparser.add_argument(\\n    '-fold',\\n    dest='fold',\\n    default=0,\\n    type=int,\\n    help='which fold we are on. useful if we want to split up a big file into multiple jobs.'\\n)\\nparser.add_argument(\\n    '-max_batch_size',\\n    dest='max_batch_size',\\n    default=None,\\n    type=int,\\n    help='max batch size. You can leave this out and we will infer one based on the number of hidden layers',\\n)\\nparser.add_argument(\\n    '-top_p',\\n    dest='top_p',\\n    default=0.95,\\n    type=float,\\n    help='p to use for top p sampling. if this isn\\\\'t none, use this for everthing'\\n)\\nparser.add_argument(\\n    '-samples',\\n    dest='samples',\\n    default=1,\\n    type=int,\\n    help='num_samples',\\n)\\n\\nargs = parser.parse_args()\\n\\nencoder = get_encoder()\\nnews_config = GroverConfig.from_json_file(args.model_config_fn)\\n\\n# We might have to split the batch into multiple chunks if the batch size is too large\\ndefault_mbs = {12: 32, 24: 16, 48: 3}\\nmax_batch_size = args.max_batch_size if args.max_batch_size is not None else default_mbs[news_config.num_hidden_layers]\\n\\n# factorize args.batch_size = (num_chunks * batch_size_per_chunk) s.t. batch_size_per_chunk < max_batch_size\\nnum_chunks = int(np.ceil(args.batch_size / max_batch_size))\\nbatch_size_per_chunk = int(np.ceil(args.batch_size / num_chunks))\\nprint(\"\\\\n~~\\\\nbatch size={}, max batch size={}, num chunks={}, batch size per chunk={}\\\\n~~\\\\n\".format(\\n    args.batch_size, max_batch_size, num_chunks, batch_size_per_chunk), flush=True)\\n\\n# This controls the top p for each generation.\\ntop_p = np.ones((num_chunks, batch_size_per_chunk), dtype=np.float32) * args.top_p\\n\\n# with open(args.metadata_fn, 'r') as f:\\n#     articles = [json.loads(l) for i, l in enumerate(f) if i % args.num_folds == args.fold]\\n\\ntf_config = tf.ConfigProto(allow_soft_placement=True)\\n\\nwith tf.Session(config=tf_config, graph=tf.Graph()) as sess:\\n    initial_context = tf.placeholder(tf.int32, [batch_size_per_chunk, None])\\n    p_for_topp = tf.placeholder(tf.float32, [batch_size_per_chunk])\\n    eos_token = tf.placeholder(tf.int32, [])\\n    tokens, probs = sample(news_config=news_config, initial_context=initial_context,\\n                           eos_token=eos_token, ignore_ids=None, p_for_topp=p_for_topp,\\n                           do_topk=False)\\n\\n    saver = tf.train.Saver()\\n    saver.restore(sess, args.model_ckpt)\\n    print('Loaded model.')\\n    text = input()\\n    while text != \"\":\\n        for i in range(args.samples):\\n            print(\"Sample,\", i + 1, \" of \", args.samples)\\n            # Let's go!\\n            encoded = _tokenize_article_pieces(encoder, text)\\n            context_formatted = []\\n            # for key in ['domain', 'date', 'authors', 'title', 'article']:\\n            #     if key != args.target:\\n            #         context_formatted.extend(article_pieces.pop(key, []))\\n            # context_formatted.append(encoder.__dict__['begin_{}'.format(args.target)])\\n            context_formatted.extend(encoded[:-1])\\n            # Format context end\\n\\n            # Indices we definitely DONT WANT TO PREDICT\\n            ignore_ids_np = np.array(encoder.special_tokens_onehot)\\n            ignore_ids_np[encoder.endoftext] = 0\\n\\n            gens = []\\n            gens_raw = []\\n            gen_probs = []\\n\\n            # article['top_ps'] = top_p.reshape(-1).tolist()\\n            for chunk_i in range(num_chunks):\\n                tokens_out, probs_out = sess.run([tokens, probs],\\n                                                 feed_dict={initial_context: [context_formatted] * batch_size_per_chunk,\\n                                                            eos_token: 60000,\\n                                                            p_for_topp: top_p[chunk_i]})\\n\\n                for t_i, p_i in zip(tokens_out, probs_out):\\n                    extraction = extract_generated_target(output_tokens=t_i, encoder=encoder, target=args.target)\\n                    gens.append(extraction['extraction'])\\n\\n            # article['gens_{}'.format(args.target)] = gens\\n            # article['gensraw_{}'.format(args.target)] = gens_raw\\n            # article['probs_{}'.format(args.target)] = gen_probs\\n\\n            # these were in there for whatever reason...\\n            # article.pop('input_ids_conditional', None)\\n            # article.pop('input_ids_unconditional', None)\\n            # f_out.write(json.dumps(article) + '\\\\n')\\n            print(gens[0])\\n        text = input()\\n\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZw6tt7qCNYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Supports repetitions with -samples <number>\n",
        "# This version supports multi line input, but colab doesn't support EOF from the stdin interface so there is no way to actaully press enter other than echoing input.\n",
        "with open('sample/contextual_generate_cli_multiline.py', 'w') as f:\n",
        "  f.write(\"\"\"\\n\\n\\nimport tensorflow as tf\\nimport numpy as np\\nimport sys\\nimport json\\nimport sys\\n\\nsys.path.append('../')\\nfrom lm.modeling import GroverModel, GroverConfig, _top_p_sample, sample\\nfrom sample.encoder import get_encoder, format_context, _tokenize_article_pieces, extract_generated_target\\nfrom tqdm import tqdm\\n\\nimport argparse\\n\\nparser = argparse.ArgumentParser(description='Contextual generation (aka given some metadata we will generate articles')\\nparser.add_argument(\\n    '-metadata_fn',\\n    dest='metadata_fn',\\n    type=str,\\n    help='Path to a JSONL containing metadata',\\n)\\nparser.add_argument(\\n    '-out_fn',\\n    dest='out_fn',\\n    type=str,\\n    help='Out jsonl, which will contain the completed jsons',\\n)\\nparser.add_argument(\\n    '-input',\\n    dest='input',\\n    type=str,\\n    help='Text to complete',\\n)\\nparser.add_argument(\\n    '-model_config_fn',\\n    dest='model_config_fn',\\n    default='../lm/configs/base.json',\\n    type=str,\\n    help='Configuration JSON for the model',\\n)\\nparser.add_argument(\\n    '-model_ckpt',\\n    dest='model_ckpt',\\n    default='../models/base/model.ckpt',\\n    type=str,\\n    help='checkpoint file for the model',\\n)\\nparser.add_argument(\\n    '-target',\\n    dest='target',\\n    default='article',\\n    type=str,\\n    help='What to generate for each item in metadata_fn. can be article (body), title, etc.',\\n)\\nparser.add_argument(\\n    '-batch_size',\\n    dest='batch_size',\\n    default=1,\\n    type=int,\\n    help='How many things to generate per context. will split into chunks if need be',\\n)\\nparser.add_argument(\\n    '-num_folds',\\n    dest='num_folds',\\n    default=1,\\n    type=int,\\n    help='Number of folds. useful if we want to split up a big file into multiple jobs.',\\n)\\nparser.add_argument(\\n    '-fold',\\n    dest='fold',\\n    default=0,\\n    type=int,\\n    help='which fold we are on. useful if we want to split up a big file into multiple jobs.'\\n)\\nparser.add_argument(\\n    '-max_batch_size',\\n    dest='max_batch_size',\\n    default=None,\\n    type=int,\\n    help='max batch size. You can leave this out and we will infer one based on the number of hidden layers',\\n)\\nparser.add_argument(\\n    '-top_p',\\n    dest='top_p',\\n    default=0.95,\\n    type=float,\\n    help='p to use for top p sampling. if this isn\\\\'t none, use this for everthing'\\n)\\nparser.add_argument(\\n    '-samples',\\n    dest='samples',\\n    default=1,\\n    type=int,\\n    help='num_samples',\\n)\\n\\nargs = parser.parse_args()\\n\\nencoder = get_encoder()\\nnews_config = GroverConfig.from_json_file(args.model_config_fn)\\n\\n# We might have to split the batch into multiple chunks if the batch size is too large\\ndefault_mbs = {12: 32, 24: 16, 48: 3}\\nmax_batch_size = args.max_batch_size if args.max_batch_size is not None else default_mbs[news_config.num_hidden_layers]\\n\\n# factorize args.batch_size = (num_chunks * batch_size_per_chunk) s.t. batch_size_per_chunk < max_batch_size\\nnum_chunks = int(np.ceil(args.batch_size / max_batch_size))\\nbatch_size_per_chunk = int(np.ceil(args.batch_size / num_chunks))\\nprint(\"\\\\n~~\\\\nbatch size={}, max batch size={}, num chunks={}, batch size per chunk={}\\\\n~~\\\\n\".format(\\n    args.batch_size, max_batch_size, num_chunks, batch_size_per_chunk), flush=True)\\n\\n# This controls the top p for each generation.\\ntop_p = np.ones((num_chunks, batch_size_per_chunk), dtype=np.float32) * args.top_p\\n\\n# with open(args.metadata_fn, 'r') as f:\\n#     articles = [json.loads(l) for i, l in enumerate(f) if i % args.num_folds == args.fold]\\n\\ntf_config = tf.ConfigProto(allow_soft_placement=True)\\n\\nwith tf.Session(config=tf_config, graph=tf.Graph()) as sess:\\n    initial_context = tf.placeholder(tf.int32, [batch_size_per_chunk, None])\\n    p_for_topp = tf.placeholder(tf.float32, [batch_size_per_chunk])\\n    eos_token = tf.placeholder(tf.int32, [])\\n    tokens, probs = sample(news_config=news_config, initial_context=initial_context,\\n                           eos_token=eos_token, ignore_ids=None, p_for_topp=p_for_topp,\\n                           do_topk=False)\\n\\n    saver = tf.train.Saver()\\n    saver.restore(sess, args.model_ckpt)\\n    print('Loaded model.')\\n    text = \"\".join(sys.stdin.readlines())\\n    for i in range(args.samples):\\n        print(\"Sample,\", i + 1, \" of \", args.samples)\\n        # Let's go!\\n        encoded = _tokenize_article_pieces(encoder, text)\\n        context_formatted = []\\n            \\n        context_formatted.extend(encoded[:-1])\\n        # Format context end\\n\\n        # Indices we definitely DONT WANT TO PREDICT\\n        ignore_ids_np = np.array(encoder.special_tokens_onehot)\\n        ignore_ids_np[encoder.endoftext] = 0\\n\\n        gens = []\\n        gens_raw = []\\n        gen_probs = []\\n\\n        # article['top_ps'] = top_p.reshape(-1).tolist()\\n        for chunk_i in range(num_chunks):\\n            tokens_out, probs_out = sess.run([tokens, probs],\\n                                             feed_dict={initial_context: [context_formatted] * batch_size_per_chunk,\\n                                                            eos_token: 60000,\\n                                                            p_for_topp: top_p[chunk_i]})\\n\\n            for t_i, p_i in zip(tokens_out, probs_out):\\n                extraction = extract_generated_target(output_tokens=t_i, encoder=encoder, target=args.target)\\n                gens.append(extraction['extraction'])\\n        print(gens[0])\\n\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50jDXTRGTTgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9bc666e-bb9b-4cbb-bf96-85f7804ac861"
      },
      "source": [
        "#THis option lets you type from colab, but you can't enter new lines\n",
        "!PYTHONPATH=$(pwd) python3 sample/contextual_generate_cli.py -model_config_fn lm/configs/mega.json -model_ckpt models/mega/model.ckpt.800000"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0823 12:51:54.145685 140496308524928 deprecation_wrapper.py:119] From /content/grover/lm/optimization_adafactor.py:88: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0823 12:51:54.249033 140496308524928 deprecation_wrapper.py:119] From /content/grover/lm/modeling.py:87: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "\n",
            "~~\n",
            "batch size=1, max batch size=3, num chunks=1, batch size per chunk=1\n",
            "~~\n",
            "\n",
            "W0823 12:51:54.249846 140496308524928 deprecation_wrapper.py:119] From sample/contextual_generate_cli.py:119: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0823 12:51:54.250112 140496308524928 deprecation_wrapper.py:119] From sample/contextual_generate_cli.py:121: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-08-23 12:51:54.265239: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-08-23 12:51:54.265503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x30032c0 executing computations on platform Host. Devices:\n",
            "2019-08-23 12:51:54.265537: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-08-23 12:51:54.278442: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-08-23 12:51:54.374147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 12:51:54.374677: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3003d40 executing computations on platform CUDA. Devices:\n",
            "2019-08-23 12:51:54.374709: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-08-23 12:51:54.375406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 12:51:54.375791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-23 12:51:54.382345: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-23 12:51:54.390559: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-08-23 12:51:54.395129: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-08-23 12:51:54.403349: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-08-23 12:51:54.413323: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-08-23 12:51:54.420820: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-08-23 12:51:54.437426: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-08-23 12:51:54.437527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 12:51:54.438070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 12:51:54.438509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-23 12:51:54.438573: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-23 12:51:54.439691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-08-23 12:51:54.439725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-08-23 12:51:54.439742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-08-23 12:51:54.440123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 12:51:54.440531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 12:51:54.440917: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-08-23 12:51:54.440970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0823 12:51:54.443311 140496308524928 deprecation_wrapper.py:119] From sample/contextual_generate_cli.py:122: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0823 12:51:54.504974 140496308524928 deprecation_wrapper.py:119] From /content/grover/lm/modeling.py:725: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0823 12:51:54.518433 140496308524928 deprecation_wrapper.py:119] From /content/grover/lm/modeling.py:490: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0823 12:51:54.607027 140496308524928 deprecation.py:323] From /content/grover/lm/modeling.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0823 12:52:05.206928 140496308524928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims` instead.\n",
            "W0823 12:52:11.244514 140496308524928 deprecation_wrapper.py:119] From sample/contextual_generate_cli.py:129: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0823 12:52:12.027129 140496308524928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loaded model.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5652, in get_controller\n",
            "    yield g\n",
            "  File \"sample/contextual_generate_cli.py\", line 132, in <module>\n",
            "    text = input()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"sample/contextual_generate_cli.py\", line 174, in <module>\n",
            "    text = input()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1619, in __exit__\n",
            "    close_thread.join(30.0)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9KrPjKkWp8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e37276b1-1ee7-448d-cb8b-4f437006180a"
      },
      "source": [
        "# newlines are  not supported well by Google Colab input so please using echo or printf to pipe your input in if you want to us enew lines\n",
        "# Just modify the text below to be what you want\n",
        "!printf \"This is D.J. Pauly D. You'll be nobody's tonight Being a Guido is a way of life.\\n\\n  It takes me 25 minutes to do my hair. I'm so fresh I got a tanning bed in my house….\\n\\n\" | PYTHONPATH=$(pwd) python3 sample/contextual_generate_cli_multiline.py -model_config_fn lm/configs/mega.json -samples 10 -model_ckpt models/mega/model.ckpt.800000"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0823 13:21:29.872443 140463287609216 deprecation_wrapper.py:119] From /content/grover/lm/optimization_adafactor.py:88: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0823 13:21:30.002852 140463287609216 deprecation_wrapper.py:119] From /content/grover/lm/modeling.py:87: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "\n",
            "~~\n",
            "batch size=1, max batch size=3, num chunks=1, batch size per chunk=1\n",
            "~~\n",
            "\n",
            "W0823 13:21:30.004668 140463287609216 deprecation_wrapper.py:119] From sample/contextual_generate_cli_multiline.py:121: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0823 13:21:30.004920 140463287609216 deprecation_wrapper.py:119] From sample/contextual_generate_cli_multiline.py:123: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-08-23 13:21:30.021256: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-08-23 13:21:30.021508: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x165f480 executing computations on platform Host. Devices:\n",
            "2019-08-23 13:21:30.021548: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-08-23 13:21:30.038949: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-08-23 13:21:30.141789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 13:21:30.142364: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5172300 executing computations on platform CUDA. Devices:\n",
            "2019-08-23 13:21:30.142401: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-08-23 13:21:30.142606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 13:21:30.143021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-23 13:21:30.147680: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-23 13:21:30.156357: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-08-23 13:21:30.162022: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-08-23 13:21:30.169001: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-08-23 13:21:30.178714: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-08-23 13:21:30.185128: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-08-23 13:21:30.198466: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-08-23 13:21:30.198597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 13:21:30.199118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 13:21:30.199477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-23 13:21:30.199545: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-23 13:21:30.200660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-08-23 13:21:30.200697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-08-23 13:21:30.200716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-08-23 13:21:30.201061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 13:21:30.201495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-23 13:21:30.201888: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-08-23 13:21:30.201943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0823 13:21:30.204173 140463287609216 deprecation_wrapper.py:119] From sample/contextual_generate_cli_multiline.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0823 13:21:30.265707 140463287609216 deprecation_wrapper.py:119] From /content/grover/lm/modeling.py:725: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0823 13:21:30.279499 140463287609216 deprecation_wrapper.py:119] From /content/grover/lm/modeling.py:490: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0823 13:21:30.380439 140463287609216 deprecation.py:323] From /content/grover/lm/modeling.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0823 13:21:41.258143 140463287609216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims` instead.\n",
            "W0823 13:21:47.462996 140463287609216 deprecation_wrapper.py:119] From sample/contextual_generate_cli_multiline.py:131: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0823 13:21:48.269111 140463287609216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loaded model.\n",
            "Sample, 1  of  10\n",
            "2019-08-23 13:22:31.894738: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "This is D.J. Pauly D. You'll be nobody's tonight Being a Guido is a way of life.\n",
            "\n",
            "  It takes me 25 minutes to do my hair. I'm so fresh I got a tanning bed in my house….\n",
            "\n",
            "\n",
            "\n",
            "OK, I know, I know. Derek. Don't get off my lawn with this nonsense. It's an innocent innocent child of facts, I'm just keeping you's address.\n",
            "\n",
            "\n",
            "\n",
            "It was the first time I had worn red lipstick, and I was all like \"bae dat…taco?\"\n",
            "\n",
            "\n",
            "\n",
            "It was the first time I had woken up at 2 A.M. and I was all like \"whoops\", and realized that nothing of my wonderful hair looked as shiny as my heads-hip.\n",
            "\n",
            "\n",
            "\n",
            "I gave up on my hair but I did get a much nicer tan.\n",
            "\n",
            "\n",
            "\n",
            "I'm still not clean and fresh, but I've become a Guido.\n",
            "\n",
            "\n",
            "\n",
            "What's a Guido you ask?\n",
            "\n",
            "\n",
            "\n",
            "A guido is an underdog, a person who refuses to be treated any differently.\n",
            "\n",
            "\n",
            "\n",
            "When I started having bouts of hair lice that were attacking and invading my bed for no reason, my friends and family were so concerned I thought I was going crazy, but after months of feeling worse, they just called me a guido.\n",
            "\n",
            "\n",
            "\n",
            "A guido just won't take crap from no one. A guido won't give it up to no one.\n",
            "\n",
            "\n",
            "\n",
            "A guido makes her own way. A guido refuses to be anyones' bad influence. A guido knows that her hair is adorable and she wouldn't ask for it back if she weren't. She wouldn't let anyone know she have lice, other than the strange women who would walk by the house if they walked by at night.\n",
            "\n",
            "\n",
            "\n",
            "A guido stays until everybody except you stops bothering her and then finally if she wants.\n",
            "\n",
            "\n",
            "\n",
            "A guido doesn't give a fuck, as long as she does, and then goes about her business.\n",
            "\n",
            "\n",
            "\n",
            "A guido never gives in to the pressure, unless it's the pressure of her family, her friends, her lover, or her Guido.\n",
            "\n",
            "\n",
            "\n",
            "I’m not sure where my roots lie, but I'm not sure where they came from, but I'm sure they're in my Guido.\n",
            "\n",
            "\n",
            "\n",
            "At first it was a weird but beautiful realization, you know, you do all this ridiculous stuff and stuff that you can get paid for, and yet you don't even realize that you're giving up all the comforts, and comfort that come with being an ordinary normal person. You just stand up straight and look at the mirror and say \"whoopie doo\", and you leave and do the same thing at night.\n",
            "\n",
            "\n",
            "\n",
            "You're taking a sabbatical, a vacation, but as far as getting shit done, you're just fucking doing what you have to do.\n",
            "\n",
            "\n",
            "\n",
            "Guido lessons are supposed to be fun, right?\n",
            "\n",
            "\n",
            "\n",
            "You teach your shit, and your childhood, and your Lifestyle to people who are into that stuff, but you also realize how hard it can be. You get embarrassed and make mistakes, but it's also alright because every once in a while someone reaches out and says \"I see what you're doing, I like you for who you are, I like you because you have my respect\".\n",
            "\n",
            "\n",
            "\n",
            "The interesting thing about it, is that it never really feels like a Guido lesson, if anyone comes along, or a Guido teacher or anyone else comes along, the Guido goes.\n",
            "\n",
            "\n",
            "\n",
            "Now, here's the stuff you should know about me.\n",
            "\n",
            "\n",
            "\n",
            "I'm an amateur singer in my days, but not in my face. I'm pretty much a girl in a wig. Hair dos a girl crazy sometimes. It makes all the jokes I tell about my tits fall…when I'm hitting a boy with a pie. It also wakes me up at 3 A.M. every time a room full of assholes scream and jump up and down.\n",
            "\n",
            "\n",
            "\n",
            "So I'm Guido it has to be done, it takes a lot of my time, it wastes a lot of time.\n",
            "\n",
            "\n",
            "\n",
            "Then, when it's done, when my Guido is finished its good for me.\n",
            "\n",
            "\n",
            "\n",
            "Who are you?\n",
            "\n",
            "\n",
            "\n",
            "You either rock it harder than me, or you just should not try.\n",
            "\n",
            "\n",
            "\n",
            "Who is the hottest girl in the world?\n",
            "\n",
            "\n",
            "\n",
            "Oh, take your pick, I'm all about talking about my top 5: Karen, UTAH, FLORIDA, BETHESDA, and GET THE FUCK UP, CHARLIE.\n",
            "\n",
            "\n",
            "\n",
            "What's your favorite meal?\n",
            "\n",
            "\n",
            "\n",
            "Off-menu, and classic, KFC chicken and hot sauce.\n",
            "\n",
            "\n",
            "\n",
            "What's your favorite toy?\n",
            "\n",
            "\n",
            "\n",
            "Sample, 2  of  10\n",
            "Traceback (most recent call last):\n",
            "  File \"sample/contextual_generate_cli_multiline.py\", line 157, in <module>\n",
            "    p_for_topp: top_p[chunk_i]})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8Z1V2j5XAZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}